{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Survivors of the Titanic \n",
    "\n",
    "This notebook looks into one of the most popular online Kaggle competition projects in building a machine learning model to predict if a passenger on the titanic would survive! \n",
    "\n",
    "For this project we're going to take my personal approach to solving ML problems: \n",
    "\n",
    " 1. Problem Definition \n",
    " 2. Data \n",
    " 3. Evaluation (Metrics)\n",
    " 4. Features \n",
    " 5. Modelling \n",
    " 6. Iterative Experimentation \n",
    " 7. Presenting Analysis\n",
    " \n",
    "# 1. Problem Definition\n",
    "In a statement, \n",
    "> Given passenger information aboard the titanic, can we predict who will survive the shipwreck?\n",
    "\n",
    "# 2. Data \n",
    "\n",
    "The data can be found at kaggle on the following link: [Titanic Data](https://www.kaggle.com/c/titanic/data)\n",
    "\n",
    "The data is split into two separate files: \n",
    " 1. train.csv (What we build the model on) \n",
    " 2. test.csv (What we test our model on) \n",
    " \n",
    "Since this is a competition on Kaggle - we will be submitting a **submissions.csv** to Kaggle - this essentially have passenger id's from the test.csv document along with a column indicating from our model if they survived or not.\n",
    "\n",
    "# 3. Evaluation \n",
    "\n",
    "Our Goal is the following: \n",
    "\n",
    ">Predict if a passenger survived the sinking of the Titanic or not. For each passenger in the test set, we must predict a 0 or 1 value if they did or did not survive. \n",
    "\n",
    "Evaluation Metric for our model is: \n",
    ">**Accuracy**\n",
    "\n",
    "Formally:\n",
    "> Accuracy is the fraction of predictions our model got right. \n",
    "\n",
    "Mathematically: \n",
    "\n",
    "$Accuracy = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$\n",
    "\n",
    "For our problem:\n",
    "> The accuracy score is the percentage of passengers you correctly predict to survive the shipwreck.\n",
    "\n",
    "# 4. Features \n",
    "\n",
    "**Data Dictionary** \n",
    "\n",
    "This allows us to describe the different features (columns) within our data, we will be using these features along with *engineering* our own later on for predicting if a passenger will survive: \n",
    "\n",
    "1. survival - Key flagging if survived \n",
    "    - 0 = No\n",
    "    - 1 = Yes\n",
    "2. pclass - Ticket class \n",
    "    - 1 = 1st Class \n",
    "    - 2 = 2nd Class \n",
    "    - 3 = 3rd Class \n",
    "3. sex - Gender \n",
    "4. Age - Age in years \n",
    "5. sibsp - # of siblings/spouses aboard the Titanic\n",
    "6. parch - # of parents/children aboard the Titanic\n",
    "7. ticket - Ticket Number\n",
    "8. fate - Passenger fare \n",
    "9. cabin - Cabin number \n",
    "10. embarked - Port of Embarkation\n",
    "    - C = Cherbourg\n",
    "    - Q = Queenstown\n",
    "    - S = Southampton\n",
    "\n",
    "Additional Note on Features \n",
    "\n",
    "> pclass: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    ">age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "> sibsp: The dataset defines family relations in this way...\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancÃ©s were ignored)\n",
    "\n",
    ">parch: The dataset defines family relations in this way...\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them.\n",
    "\n",
    "\n",
    "\n",
    "# 5. Modelling \n",
    "\n",
    "Since the problem is a Yes/No outcome dictating if a passenger would survive or not, we can immediately identify that this is a supervised machine learning problem within the area of classification. \n",
    "\n",
    "We'll employ the following methods and models to predict a binary (0,1) outcome: \n",
    "\n",
    "1. Logistic Regression (Baseline Model - for comparing our later models score to) \n",
    "2. Regularized Logistic Regression (Lasso, Ridge and Elastic Net)\n",
    "3. Random Forest Classifier\n",
    "4. XGBoost Classifier\n",
    "\n",
    "# 6. Iterative Experimentation \n",
    "\n",
    "Upon creating our baseline model of Logistic Regression, we will then improve upon this model using regularized methods along with ensemble classifiers (including gradient boosted trees). During this stage we will be using hyperparameter tuning and specialized feature engineering to see if we can improve upon the model evaluation metric: **Accuracy**. \n",
    "\n",
    "Once we have finalised the best model to use, we will generate the submitted data of our predictions along with pickling the model for future model analysis. \n",
    "\n",
    "# 7. Presenting Analysis\n",
    "\n",
    "Once our model has been finalised and chosen - we will take key select findings from this project and place them into a singular notebook explaining the methods and techniques that enabled us to pick the best models possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
